{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e2907d-0d22-485d-b7d0-d9b2f7db17c9",
   "metadata": {},
   "source": [
    "# Neural Network Model for Tabular Data in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b176a98b-1b82-4c9e-9ac6-44cdf30dfe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b84386-b3cc-4974-a00e-384616da6420",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"avila-tr.txt\", header=None)\n",
    "test_df = pd.read_csv(\"avila-ts.txt\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11c59e86-3ea9-473a-b339-92da393619f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.266074</td>\n",
       "      <td>-0.165620</td>\n",
       "      <td>0.320980</td>\n",
       "      <td>0.483299</td>\n",
       "      <td>0.172340</td>\n",
       "      <td>0.273364</td>\n",
       "      <td>0.371178</td>\n",
       "      <td>0.929823</td>\n",
       "      <td>0.251173</td>\n",
       "      <td>0.159345</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.130292</td>\n",
       "      <td>0.870736</td>\n",
       "      <td>-3.210528</td>\n",
       "      <td>0.062493</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>1.436060</td>\n",
       "      <td>1.465940</td>\n",
       "      <td>0.636203</td>\n",
       "      <td>0.282354</td>\n",
       "      <td>0.515587</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.116585</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.068476</td>\n",
       "      <td>-0.783147</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>0.439463</td>\n",
       "      <td>-0.081827</td>\n",
       "      <td>-0.888236</td>\n",
       "      <td>-0.123005</td>\n",
       "      <td>0.582939</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.031541</td>\n",
       "      <td>0.297600</td>\n",
       "      <td>-3.210528</td>\n",
       "      <td>-0.583590</td>\n",
       "      <td>-0.721442</td>\n",
       "      <td>-0.307984</td>\n",
       "      <td>0.710932</td>\n",
       "      <td>1.051693</td>\n",
       "      <td>0.594169</td>\n",
       "      <td>-0.533994</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.229043</td>\n",
       "      <td>0.807926</td>\n",
       "      <td>-0.052442</td>\n",
       "      <td>0.082634</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>0.148790</td>\n",
       "      <td>0.635431</td>\n",
       "      <td>0.051062</td>\n",
       "      <td>0.032902</td>\n",
       "      <td>-0.086652</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.266074 -0.165620  0.320980  0.483299  0.172340  0.273364  0.371178   \n",
       "1  0.130292  0.870736 -3.210528  0.062493  0.261718  1.436060  1.465940   \n",
       "2 -0.116585  0.069915  0.068476 -0.783147  0.261718  0.439463 -0.081827   \n",
       "3  0.031541  0.297600 -3.210528 -0.583590 -0.721442 -0.307984  0.710932   \n",
       "4  0.229043  0.807926 -0.052442  0.082634  0.261718  0.148790  0.635431   \n",
       "\n",
       "         7         8         9  10  \n",
       "0  0.929823  0.251173  0.159345  A  \n",
       "1  0.636203  0.282354  0.515587  A  \n",
       "2 -0.888236 -0.123005  0.582939  A  \n",
       "3  1.051693  0.594169 -0.533994  A  \n",
       "4  0.051062  0.032902 -0.086652  F  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb1edc3-2c0b-456f-99a7-e948c05d7f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['col_'+str(j+1) for j in range(train_df.shape[1]-1)]\n",
    "indep_cols = col_names.copy()\n",
    "col_names.append('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bdc183b-6242-42f4-8c51-e5056ca4e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns = col_names\n",
    "test_df.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5032b6b-2ebf-442e-aac6-e7da3614b1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "      <th>col_9</th>\n",
       "      <th>col_10</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.266074</td>\n",
       "      <td>-0.165620</td>\n",
       "      <td>0.320980</td>\n",
       "      <td>0.483299</td>\n",
       "      <td>0.172340</td>\n",
       "      <td>0.273364</td>\n",
       "      <td>0.371178</td>\n",
       "      <td>0.929823</td>\n",
       "      <td>0.251173</td>\n",
       "      <td>0.159345</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.130292</td>\n",
       "      <td>0.870736</td>\n",
       "      <td>-3.210528</td>\n",
       "      <td>0.062493</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>1.436060</td>\n",
       "      <td>1.465940</td>\n",
       "      <td>0.636203</td>\n",
       "      <td>0.282354</td>\n",
       "      <td>0.515587</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.116585</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.068476</td>\n",
       "      <td>-0.783147</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>0.439463</td>\n",
       "      <td>-0.081827</td>\n",
       "      <td>-0.888236</td>\n",
       "      <td>-0.123005</td>\n",
       "      <td>0.582939</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.031541</td>\n",
       "      <td>0.297600</td>\n",
       "      <td>-3.210528</td>\n",
       "      <td>-0.583590</td>\n",
       "      <td>-0.721442</td>\n",
       "      <td>-0.307984</td>\n",
       "      <td>0.710932</td>\n",
       "      <td>1.051693</td>\n",
       "      <td>0.594169</td>\n",
       "      <td>-0.533994</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.229043</td>\n",
       "      <td>0.807926</td>\n",
       "      <td>-0.052442</td>\n",
       "      <td>0.082634</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>0.148790</td>\n",
       "      <td>0.635431</td>\n",
       "      <td>0.051062</td>\n",
       "      <td>0.032902</td>\n",
       "      <td>-0.086652</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      col_1     col_2     col_3     col_4     col_5     col_6     col_7  \\\n",
       "0  0.266074 -0.165620  0.320980  0.483299  0.172340  0.273364  0.371178   \n",
       "1  0.130292  0.870736 -3.210528  0.062493  0.261718  1.436060  1.465940   \n",
       "2 -0.116585  0.069915  0.068476 -0.783147  0.261718  0.439463 -0.081827   \n",
       "3  0.031541  0.297600 -3.210528 -0.583590 -0.721442 -0.307984  0.710932   \n",
       "4  0.229043  0.807926 -0.052442  0.082634  0.261718  0.148790  0.635431   \n",
       "\n",
       "      col_8     col_9    col_10  y  \n",
       "0  0.929823  0.251173  0.159345  A  \n",
       "1  0.636203  0.282354  0.515587  A  \n",
       "2 -0.888236 -0.123005  0.582939  A  \n",
       "3  1.051693  0.594169 -0.533994  A  \n",
       "4  0.051062  0.032902 -0.086652  F  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e7fdee5-3d2c-49fb-8782-a0cee98ced42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train_df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7285d3c6-3662-4cf1-bf75-c6b13fee30eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'W', 'X', 'Y'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86dbf79c-e22c-4434-9aec-9fb6d807a8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.transform(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8eec30f-9d59-4f10-9e62-e89dfa9e648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['y_enc'] = le.transform(train_df['y'])\n",
    "test_df['y_enc'] = le.transform(test_df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19151f51-4525-4daa-bcc7-7db9de2b1108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "      <th>col_9</th>\n",
       "      <th>col_10</th>\n",
       "      <th>y</th>\n",
       "      <th>y_enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.266074</td>\n",
       "      <td>-0.165620</td>\n",
       "      <td>0.320980</td>\n",
       "      <td>0.483299</td>\n",
       "      <td>0.172340</td>\n",
       "      <td>0.273364</td>\n",
       "      <td>0.371178</td>\n",
       "      <td>0.929823</td>\n",
       "      <td>0.251173</td>\n",
       "      <td>0.159345</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.130292</td>\n",
       "      <td>0.870736</td>\n",
       "      <td>-3.210528</td>\n",
       "      <td>0.062493</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>1.436060</td>\n",
       "      <td>1.465940</td>\n",
       "      <td>0.636203</td>\n",
       "      <td>0.282354</td>\n",
       "      <td>0.515587</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.116585</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.068476</td>\n",
       "      <td>-0.783147</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>0.439463</td>\n",
       "      <td>-0.081827</td>\n",
       "      <td>-0.888236</td>\n",
       "      <td>-0.123005</td>\n",
       "      <td>0.582939</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.031541</td>\n",
       "      <td>0.297600</td>\n",
       "      <td>-3.210528</td>\n",
       "      <td>-0.583590</td>\n",
       "      <td>-0.721442</td>\n",
       "      <td>-0.307984</td>\n",
       "      <td>0.710932</td>\n",
       "      <td>1.051693</td>\n",
       "      <td>0.594169</td>\n",
       "      <td>-0.533994</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.229043</td>\n",
       "      <td>0.807926</td>\n",
       "      <td>-0.052442</td>\n",
       "      <td>0.082634</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>0.148790</td>\n",
       "      <td>0.635431</td>\n",
       "      <td>0.051062</td>\n",
       "      <td>0.032902</td>\n",
       "      <td>-0.086652</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      col_1     col_2     col_3     col_4     col_5     col_6     col_7  \\\n",
       "0  0.266074 -0.165620  0.320980  0.483299  0.172340  0.273364  0.371178   \n",
       "1  0.130292  0.870736 -3.210528  0.062493  0.261718  1.436060  1.465940   \n",
       "2 -0.116585  0.069915  0.068476 -0.783147  0.261718  0.439463 -0.081827   \n",
       "3  0.031541  0.297600 -3.210528 -0.583590 -0.721442 -0.307984  0.710932   \n",
       "4  0.229043  0.807926 -0.052442  0.082634  0.261718  0.148790  0.635431   \n",
       "\n",
       "      col_8     col_9    col_10  y  y_enc  \n",
       "0  0.929823  0.251173  0.159345  A      0  \n",
       "1  0.636203  0.282354  0.515587  A      0  \n",
       "2 -0.888236 -0.123005  0.582939  A      0  \n",
       "3  1.051693  0.594169 -0.533994  A      0  \n",
       "4  0.051062  0.032902 -0.086652  F      5  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a824340a-00f9-44ce-815c-50ae69b4a5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10430, 12), (10437, 12))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a296628c-61a1-463d-8013-4097b454fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvilaDataset(Dataset):\n",
    "    def __init__(self, data_frame, indep_cols, dep_col):\n",
    "        data_frame = data_frame.copy()\n",
    "        self.x = data_frame.loc[:, indep_cols].copy().values.astype(np.float32)        \n",
    "        self.y = data_frame.loc[:, dep_col].copy().values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2de6ddc8-daba-4f2a-9056-42514e167164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation dataloaders\n",
    "train_ds = AvilaDataset(data_frame=train_df, indep_cols=indep_cols, dep_col='y_enc')\n",
    "valid_ds = AvilaDataset(data_frame=test_df, indep_cols=indep_cols, dep_col='y_enc')\n",
    "\n",
    "# Should be some exponent of 2 (128, 256)\n",
    "batch_size = 256\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bb6a506-1634-4563-a826-2a4f00d27396",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff129f77-5839-425b-9eb9-a10bc3dd9823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Structure\n",
    "# Try to remove relu from the last layer\n",
    "# Don't put drop and bn in the name network\n",
    "\n",
    "class AvilaNNModelV1(nn.Module):\n",
    "    \n",
    "    def __init__(self, comps, classes):\n",
    "        super().__init__()\n",
    "        self.comps = comps\n",
    "        self.classes = classes\n",
    "        self.bn1 = nn.BatchNorm1d(self.comps)\n",
    "        self.lin1 = nn.Linear(self.comps, 50)\n",
    "        self.drops = nn.Dropout(0.3)\n",
    "        self.bn2 = nn.BatchNorm1d(50)\n",
    "        self.lin2 = nn.Linear(50, 70)\n",
    "        self.bn3 = nn.BatchNorm1d(70)\n",
    "        self.lin3 = nn.Linear(70, self.classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn3(x)\n",
    "#         logits = F.relu(self.lin3(x))\n",
    "        logits = self.lin3(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41738eef-706a-4393-a306-b70a7b341a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AvilaNNModelV1(10, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d325c0f7-871f-4817-9d5d-58364c5106f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d839095f-269f-48dd-9d2d-a35d83a1762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9191297-61f1-4263-980b-f01376f0bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    # model.train() # Having this line prevents my model accuracy going beyond 70%\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    train_loss /= num_batches\n",
    "    correct /= size\n",
    "    return 100*correct, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bcc368c-3efd-4bcd-88fb-b1ca0c863b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    return 100*correct, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc9e2bc2-349b-4472-b83b-adabe9740455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Accuracy: 25.4%, Train Loss: 2.344850, Test Accuracy: 49.9%, Test loss: 1.937866\n",
      "Epoch 2 Train Accuracy: 55.1%, Train Loss: 1.557557, Test Accuracy: 57.0%, Test loss: 1.301057\n",
      "Epoch 3 Train Accuracy: 59.5%, Train Loss: 1.211687, Test Accuracy: 60.7%, Test loss: 1.149702\n",
      "Epoch 4 Train Accuracy: 62.3%, Train Loss: 1.092546, Test Accuracy: 63.1%, Test loss: 1.069381\n",
      "Epoch 5 Train Accuracy: 64.1%, Train Loss: 1.019719, Test Accuracy: 64.7%, Test loss: 1.005361\n",
      "Epoch 6 Train Accuracy: 65.8%, Train Loss: 0.957880, Test Accuracy: 65.7%, Test loss: 0.951836\n",
      "Epoch 7 Train Accuracy: 67.1%, Train Loss: 0.904176, Test Accuracy: 67.2%, Test loss: 0.906212\n",
      "Epoch 8 Train Accuracy: 68.4%, Train Loss: 0.864039, Test Accuracy: 68.3%, Test loss: 0.868830\n",
      "Epoch 9 Train Accuracy: 69.7%, Train Loss: 0.828271, Test Accuracy: 69.3%, Test loss: 0.841738\n",
      "Epoch 10 Train Accuracy: 71.0%, Train Loss: 0.793414, Test Accuracy: 70.0%, Test loss: 0.812738\n",
      "Epoch 11 Train Accuracy: 71.4%, Train Loss: 0.766084, Test Accuracy: 70.5%, Test loss: 0.784648\n",
      "Epoch 12 Train Accuracy: 72.3%, Train Loss: 0.739169, Test Accuracy: 70.9%, Test loss: 0.763855\n",
      "Epoch 13 Train Accuracy: 72.6%, Train Loss: 0.714158, Test Accuracy: 71.9%, Test loss: 0.740645\n",
      "Epoch 14 Train Accuracy: 73.1%, Train Loss: 0.694715, Test Accuracy: 72.7%, Test loss: 0.723264\n",
      "Epoch 15 Train Accuracy: 73.7%, Train Loss: 0.675602, Test Accuracy: 72.9%, Test loss: 0.712496\n",
      "Epoch 16 Train Accuracy: 74.7%, Train Loss: 0.657238, Test Accuracy: 73.5%, Test loss: 0.694199\n",
      "Epoch 17 Train Accuracy: 75.1%, Train Loss: 0.638919, Test Accuracy: 74.1%, Test loss: 0.677760\n",
      "Epoch 18 Train Accuracy: 75.4%, Train Loss: 0.624061, Test Accuracy: 74.3%, Test loss: 0.670413\n",
      "Epoch 19 Train Accuracy: 76.6%, Train Loss: 0.610776, Test Accuracy: 74.6%, Test loss: 0.658273\n",
      "Epoch 20 Train Accuracy: 76.5%, Train Loss: 0.598119, Test Accuracy: 75.0%, Test loss: 0.651112\n",
      "Epoch 21 Train Accuracy: 77.2%, Train Loss: 0.584079, Test Accuracy: 75.9%, Test loss: 0.634100\n",
      "Epoch 22 Train Accuracy: 77.3%, Train Loss: 0.570888, Test Accuracy: 75.9%, Test loss: 0.631541\n",
      "Epoch 23 Train Accuracy: 77.7%, Train Loss: 0.560292, Test Accuracy: 75.9%, Test loss: 0.619609\n",
      "Epoch 24 Train Accuracy: 78.7%, Train Loss: 0.547968, Test Accuracy: 75.9%, Test loss: 0.610818\n",
      "Epoch 25 Train Accuracy: 78.8%, Train Loss: 0.539862, Test Accuracy: 76.5%, Test loss: 0.607508\n",
      "Epoch 26 Train Accuracy: 79.2%, Train Loss: 0.526372, Test Accuracy: 77.5%, Test loss: 0.588822\n",
      "Epoch 27 Train Accuracy: 79.5%, Train Loss: 0.516775, Test Accuracy: 77.0%, Test loss: 0.593443\n",
      "Epoch 28 Train Accuracy: 80.3%, Train Loss: 0.507006, Test Accuracy: 77.5%, Test loss: 0.581089\n",
      "Epoch 29 Train Accuracy: 80.3%, Train Loss: 0.497635, Test Accuracy: 78.0%, Test loss: 0.564303\n",
      "Epoch 30 Train Accuracy: 80.8%, Train Loss: 0.488753, Test Accuracy: 78.1%, Test loss: 0.568081\n",
      "Epoch 31 Train Accuracy: 81.3%, Train Loss: 0.481147, Test Accuracy: 78.7%, Test loss: 0.557470\n",
      "Epoch 32 Train Accuracy: 81.5%, Train Loss: 0.476103, Test Accuracy: 78.8%, Test loss: 0.546055\n",
      "Epoch 33 Train Accuracy: 82.1%, Train Loss: 0.463886, Test Accuracy: 79.5%, Test loss: 0.538936\n",
      "Epoch 34 Train Accuracy: 82.5%, Train Loss: 0.452402, Test Accuracy: 79.5%, Test loss: 0.534809\n",
      "Epoch 35 Train Accuracy: 82.8%, Train Loss: 0.444239, Test Accuracy: 79.8%, Test loss: 0.529771\n",
      "Epoch 36 Train Accuracy: 83.2%, Train Loss: 0.438253, Test Accuracy: 80.1%, Test loss: 0.521613\n",
      "Epoch 37 Train Accuracy: 83.3%, Train Loss: 0.431349, Test Accuracy: 79.7%, Test loss: 0.522864\n",
      "Epoch 38 Train Accuracy: 83.7%, Train Loss: 0.423403, Test Accuracy: 80.3%, Test loss: 0.512067\n",
      "Epoch 39 Train Accuracy: 83.9%, Train Loss: 0.413778, Test Accuracy: 80.7%, Test loss: 0.512472\n",
      "Epoch 40 Train Accuracy: 84.0%, Train Loss: 0.414531, Test Accuracy: 81.0%, Test loss: 0.505629\n",
      "Epoch 41 Train Accuracy: 84.5%, Train Loss: 0.402583, Test Accuracy: 81.4%, Test loss: 0.497334\n",
      "Epoch 42 Train Accuracy: 84.6%, Train Loss: 0.394457, Test Accuracy: 81.5%, Test loss: 0.494750\n",
      "Epoch 43 Train Accuracy: 85.1%, Train Loss: 0.390270, Test Accuracy: 81.5%, Test loss: 0.492834\n",
      "Epoch 44 Train Accuracy: 85.0%, Train Loss: 0.384232, Test Accuracy: 81.2%, Test loss: 0.495903\n",
      "Epoch 45 Train Accuracy: 85.6%, Train Loss: 0.378390, Test Accuracy: 81.9%, Test loss: 0.478240\n",
      "Epoch 46 Train Accuracy: 85.8%, Train Loss: 0.368042, Test Accuracy: 82.6%, Test loss: 0.475512\n",
      "Epoch 47 Train Accuracy: 85.6%, Train Loss: 0.363166, Test Accuracy: 82.1%, Test loss: 0.477468\n",
      "Epoch 48 Train Accuracy: 85.9%, Train Loss: 0.363780, Test Accuracy: 82.4%, Test loss: 0.473284\n",
      "Epoch 49 Train Accuracy: 86.1%, Train Loss: 0.356346, Test Accuracy: 82.6%, Test loss: 0.469685\n",
      "Epoch 50 Train Accuracy: 86.4%, Train Loss: 0.349606, Test Accuracy: 83.2%, Test loss: 0.453198\n",
      "Epoch 51 Train Accuracy: 86.6%, Train Loss: 0.344848, Test Accuracy: 82.8%, Test loss: 0.458350\n",
      "Epoch 52 Train Accuracy: 86.8%, Train Loss: 0.339642, Test Accuracy: 83.5%, Test loss: 0.447393\n",
      "Epoch 53 Train Accuracy: 87.3%, Train Loss: 0.330676, Test Accuracy: 83.5%, Test loss: 0.445441\n",
      "Epoch 54 Train Accuracy: 87.5%, Train Loss: 0.330115, Test Accuracy: 83.5%, Test loss: 0.446121\n",
      "Epoch 55 Train Accuracy: 87.5%, Train Loss: 0.324298, Test Accuracy: 84.0%, Test loss: 0.439688\n",
      "Epoch 56 Train Accuracy: 87.6%, Train Loss: 0.322510, Test Accuracy: 84.2%, Test loss: 0.428059\n",
      "Epoch 57 Train Accuracy: 88.2%, Train Loss: 0.310235, Test Accuracy: 84.2%, Test loss: 0.432450\n",
      "Epoch 58 Train Accuracy: 88.0%, Train Loss: 0.311034, Test Accuracy: 83.4%, Test loss: 0.438443\n",
      "Epoch 59 Train Accuracy: 88.3%, Train Loss: 0.310156, Test Accuracy: 84.5%, Test loss: 0.422117\n",
      "Epoch 60 Train Accuracy: 88.4%, Train Loss: 0.304646, Test Accuracy: 84.7%, Test loss: 0.417651\n",
      "Epoch 61 Train Accuracy: 89.1%, Train Loss: 0.294477, Test Accuracy: 84.7%, Test loss: 0.415586\n",
      "Epoch 62 Train Accuracy: 88.9%, Train Loss: 0.297144, Test Accuracy: 84.8%, Test loss: 0.415087\n",
      "Epoch 63 Train Accuracy: 89.2%, Train Loss: 0.287564, Test Accuracy: 85.0%, Test loss: 0.410038\n",
      "Epoch 64 Train Accuracy: 89.5%, Train Loss: 0.285740, Test Accuracy: 85.3%, Test loss: 0.404295\n",
      "Epoch 65 Train Accuracy: 89.4%, Train Loss: 0.281763, Test Accuracy: 85.1%, Test loss: 0.405992\n",
      "Epoch 66 Train Accuracy: 89.4%, Train Loss: 0.280983, Test Accuracy: 85.3%, Test loss: 0.414429\n",
      "Epoch 67 Train Accuracy: 89.5%, Train Loss: 0.280484, Test Accuracy: 84.9%, Test loss: 0.415578\n",
      "Epoch 68 Train Accuracy: 89.5%, Train Loss: 0.273680, Test Accuracy: 85.4%, Test loss: 0.402632\n",
      "Epoch 69 Train Accuracy: 89.5%, Train Loss: 0.272084, Test Accuracy: 85.8%, Test loss: 0.395393\n",
      "Epoch 70 Train Accuracy: 90.6%, Train Loss: 0.260286, Test Accuracy: 86.4%, Test loss: 0.388683\n",
      "Epoch 71 Train Accuracy: 90.2%, Train Loss: 0.263907, Test Accuracy: 85.9%, Test loss: 0.403365\n",
      "Epoch 72 Train Accuracy: 90.0%, Train Loss: 0.267561, Test Accuracy: 85.8%, Test loss: 0.399598\n",
      "Epoch 73 Train Accuracy: 90.3%, Train Loss: 0.256129, Test Accuracy: 86.2%, Test loss: 0.389513\n",
      "Epoch 74 Train Accuracy: 90.5%, Train Loss: 0.252328, Test Accuracy: 86.5%, Test loss: 0.385375\n",
      "Epoch 75 Train Accuracy: 91.0%, Train Loss: 0.247910, Test Accuracy: 86.4%, Test loss: 0.380780\n",
      "Epoch 76 Train Accuracy: 91.1%, Train Loss: 0.241813, Test Accuracy: 86.8%, Test loss: 0.375602\n",
      "Epoch 77 Train Accuracy: 91.2%, Train Loss: 0.242751, Test Accuracy: 86.6%, Test loss: 0.380999\n",
      "Epoch 78 Train Accuracy: 90.3%, Train Loss: 0.273896, Test Accuracy: 85.1%, Test loss: 0.402028\n",
      "Epoch 79 Train Accuracy: 90.9%, Train Loss: 0.246319, Test Accuracy: 86.5%, Test loss: 0.380535\n",
      "Epoch 80 Train Accuracy: 91.1%, Train Loss: 0.238442, Test Accuracy: 86.8%, Test loss: 0.378688\n",
      "Epoch 81 Train Accuracy: 91.1%, Train Loss: 0.233800, Test Accuracy: 86.7%, Test loss: 0.375891\n",
      "Epoch 82 Train Accuracy: 91.5%, Train Loss: 0.233415, Test Accuracy: 86.9%, Test loss: 0.375273\n",
      "Epoch 83 Train Accuracy: 91.8%, Train Loss: 0.225899, Test Accuracy: 87.0%, Test loss: 0.372539\n",
      "Epoch 84 Train Accuracy: 91.4%, Train Loss: 0.224325, Test Accuracy: 86.8%, Test loss: 0.371698\n",
      "Epoch 85 Train Accuracy: 91.8%, Train Loss: 0.226013, Test Accuracy: 87.1%, Test loss: 0.369273\n",
      "Epoch 86 Train Accuracy: 91.7%, Train Loss: 0.221194, Test Accuracy: 87.0%, Test loss: 0.373410\n",
      "Epoch 87 Train Accuracy: 91.9%, Train Loss: 0.218477, Test Accuracy: 87.0%, Test loss: 0.368910\n",
      "Epoch 88 Train Accuracy: 92.1%, Train Loss: 0.213839, Test Accuracy: 86.5%, Test loss: 0.380880\n",
      "Epoch 89 Train Accuracy: 91.9%, Train Loss: 0.214752, Test Accuracy: 87.3%, Test loss: 0.373772\n",
      "Epoch 90 Train Accuracy: 92.0%, Train Loss: 0.213580, Test Accuracy: 87.3%, Test loss: 0.371478\n",
      "Epoch 91 Train Accuracy: 92.1%, Train Loss: 0.211515, Test Accuracy: 87.7%, Test loss: 0.362622\n",
      "Epoch 92 Train Accuracy: 92.6%, Train Loss: 0.205350, Test Accuracy: 87.2%, Test loss: 0.369964\n",
      "Epoch 93 Train Accuracy: 92.1%, Train Loss: 0.204623, Test Accuracy: 87.9%, Test loss: 0.356280\n",
      "Epoch 94 Train Accuracy: 92.6%, Train Loss: 0.203012, Test Accuracy: 87.7%, Test loss: 0.360309\n",
      "Epoch 95 Train Accuracy: 92.8%, Train Loss: 0.199114, Test Accuracy: 87.6%, Test loss: 0.372211\n",
      "Epoch 96 Train Accuracy: 92.7%, Train Loss: 0.198287, Test Accuracy: 87.8%, Test loss: 0.360009\n",
      "Epoch 97 Train Accuracy: 92.8%, Train Loss: 0.197926, Test Accuracy: 87.6%, Test loss: 0.371872\n",
      "Epoch 98 Train Accuracy: 92.8%, Train Loss: 0.199822, Test Accuracy: 87.4%, Test loss: 0.367583\n",
      "Epoch 99 Train Accuracy: 92.5%, Train Loss: 0.197822, Test Accuracy: 87.5%, Test loss: 0.374642\n",
      "Epoch 100 Train Accuracy: 92.9%, Train Loss: 0.194380, Test Accuracy: 87.4%, Test loss: 0.367180\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "best_loss = 1000\n",
    "for t in range(epochs):\n",
    "    train_acc, train_loss = train(train_dl, model, loss_fn, optimizer)\n",
    "    test_acc, test_loss = test(valid_dl, model, loss_fn)\n",
    "    if test_loss < best_loss:Welcome\n",
    "        # save model checkpoint\n",
    "        best_epoch = t + 1\n",
    "        torch.save({'epochs': best_epoch,\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict()},\n",
    "                   \"avila_chkpoint.pth\")\n",
    "        best_loss = test_loss\n",
    "    print(f\"Epoch {t+1} Train Accuracy: {train_acc:>0.1f}%, Train Loss: {train_loss:>8f}, Test Accuracy: {test_acc:>0.1f}%, Test loss: {test_loss:>8f}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b9e9689-3837-48f6-b72e-4d8671bb24f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 93\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "best_model = AvilaNNModelV1(10, 12)\n",
    "checkpoint = torch.load(\"avila_chkpoint.pth\")\n",
    "best_model.load_state_dict(checkpoint['model'])\n",
    "best_model.eval()\n",
    "print(f\"Loaded model from epoch {checkpoint['epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d9a67f0-bbd5-49ff-9f79-a686f0ad23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted values from the validation data loader using the best model\n",
    "preds = []\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, y in valid_dl:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        out = best_model(X)        \n",
    "        prob = F.softmax(out, dim=1)\n",
    "        preds.append(prob)\n",
    "\n",
    "# Concatenate the batch outputs of probabilities\n",
    "for j in range(len(preds)):\n",
    "    if j == 0:\n",
    "        pred_array = preds[j]\n",
    "    else:\n",
    "        pred_array = torch.cat((pred_array, preds[j]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "037bef2a-7232-4482-9ff9-126ea3861a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted class based on the max probability\n",
    "pred_array = pred_array.numpy()\n",
    "pred_df = pd.DataFrame(pred_array, columns=le.classes_)\n",
    "pred_df['pred_enc'] = pred_array.argmax(axis=1)\n",
    "pred_df['pred_class'] = le.inverse_transform(pred_df['pred_enc'])\n",
    "test_df['pred_class'] = pred_df['pred_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bd3d7ae-b0df-4fb2-a98f-7c04fa9a1878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>pred_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y pred_class\n",
       "0  W          W\n",
       "1  A          I\n",
       "2  I          I\n",
       "3  E          E\n",
       "4  A          A"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[['y', 'pred_class']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3b0dd4a-5428-4c4a-bd0f-9461fafb93d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save({'epochs': best_epoch,\n",
    "            'model': best_model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()},\n",
    "            \"avila_nn1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef160b2-679a-4a90-b9ab-da427d8b25f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model2 = AvilaNNModelV1(10, 12)\n",
    "checkpoint = torch.load(\"avila_nn1.pth\")\n",
    "model2.load_state_dict(checkpoint['model'])\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1283433-d4cd-45e6-a01b-27923fed3961",
   "metadata": {},
   "source": [
    "### Convert model to onnx format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc7abf5a-c41f-4294-99b8-1bf4f4e40496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37e3e5ee-20d9-4dbb-afe8-2827e121839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input to the model\n",
    "i = 1\n",
    "for X, y in valid_dl:\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    if i == 1:\n",
    "        break\n",
    "\n",
    "obs = X[0]\n",
    "obs = obs.view(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5696f4bf-5322-4b7c-a33c-64713b2189c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    obs_out = best_model(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ecc70ed-66b7-4b51-9b30-76b367832ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input : Float(1, 10, strides=[10, 1], requires_grad=0, device=cpu),\n",
      "      %bn1.weight : Float(10, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bn1.bias : Float(10, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bn1.running_mean : Float(10, strides=[1], requires_grad=0, device=cpu),\n",
      "      %bn1.running_var : Float(10, strides=[1], requires_grad=0, device=cpu),\n",
      "      %lin1.weight : Float(50, 10, strides=[10, 1], requires_grad=1, device=cpu),\n",
      "      %lin1.bias : Float(50, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bn2.weight : Float(50, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bn2.bias : Float(50, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bn2.running_mean : Float(50, strides=[1], requires_grad=0, device=cpu),\n",
      "      %bn2.running_var : Float(50, strides=[1], requires_grad=0, device=cpu),\n",
      "      %lin2.weight : Float(70, 50, strides=[50, 1], requires_grad=1, device=cpu),\n",
      "      %lin2.bias : Float(70, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bn3.weight : Float(70, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bn3.bias : Float(70, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bn3.running_mean : Float(70, strides=[1], requires_grad=0, device=cpu),\n",
      "      %bn3.running_var : Float(70, strides=[1], requires_grad=0, device=cpu),\n",
      "      %lin3.weight : Float(12, 70, strides=[70, 1], requires_grad=1, device=cpu),\n",
      "      %lin3.bias : Float(12, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %22 : Float(1, 10, strides=[10, 1], requires_grad=1, device=cpu) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002](%input, %bn1.weight, %bn1.bias, %bn1.running_mean, %bn1.running_var) # /home/indrajit/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py:2149:0\n",
      "  %23 : Float(1, 50, strides=[50, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%22, %lin1.weight, %lin1.bias) # /home/indrajit/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py:1753:0\n",
      "  %24 : Float(1, 50, strides=[50, 1], requires_grad=1, device=cpu) = onnx::Relu(%23) # /home/indrajit/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py:1076:0\n",
      "  %25 : Float(1, 50, strides=[50, 1], requires_grad=1, device=cpu) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002](%24, %bn2.weight, %bn2.bias, %bn2.running_mean, %bn2.running_var) # /home/indrajit/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py:2149:0\n",
      "  %26 : Float(1, 70, strides=[70, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%25, %lin2.weight, %lin2.bias) # /home/indrajit/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py:1753:0\n",
      "  %27 : Float(1, 70, strides=[70, 1], requires_grad=1, device=cpu) = onnx::Relu(%26) # /home/indrajit/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py:1076:0\n",
      "  %28 : Float(1, 70, strides=[70, 1], requires_grad=1, device=cpu) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002](%27, %bn3.weight, %bn3.bias, %bn3.running_mean, %bn3.running_var) # /home/indrajit/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py:2149:0\n",
      "  %output : Float(1, 12, strides=[12, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%28, %lin3.weight, %lin3.bias) # /home/indrajit/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py:1753:0\n",
      "  return (%output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Export the model\n",
    "torch.onnx.export(best_model,               # model being run\n",
    "                  obs,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"avila_nn1.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97e13ef-8220-47f2-8dc4-c4862ada2949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
